{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Sentiment Analysis Part 3\n",
    "## Optimization with Grid Search\n",
    "\n",
    "In this notebook, I intend to find more optimal parameters using Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "# Pad the small reviews\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: The below code might run for months!\n",
    "\n",
    "While searching the parameter space, I noticed that some epochs did extremely well on the training data, however, overall model accuracy changed very little from our original author's results. A quick computation of the number of parameters below shows that we have 7 * 2 * 6 * 3 * 8 = 2016 combinations of parameters. Given that the epoch=2 setting takes at least 2+ minutes and the epoch=10 setting was taking 20-90 minutes in my test run, we could be at this a very long time. Most of the tests did not return better than 88-89% accuracy. This lead me to wonder if (1) this type of CNN is the best approach and (2) if there was a more efficient way to search the parameter space (we are training advanced optimization algorithms afterall, there should be a non-brute force approach, right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(pool_length, nb_epoch, hidden_layer_size=1, init_mode='uniform'):\n",
    "    \n",
    "    #print(\"Testing new model\")\n",
    "    \n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "    model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_layer_size, init=init_mode, activation='relu'))\n",
    "    model.add(Dense(1, init=init_mode, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100, 128]\n",
    "epochs = [2, 10]\n",
    "pool_length = [2, 5, 10, 20, 32, 500]\n",
    "hidden_layer_size = [32,250, 500]\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, nb_epoch=epochs, pool_length=pool_length, hidden_layer_size=hidden_layer_size, init_mode=init_mode)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, verbose=10)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "First, I discovered that the world record was held by a company who used an RNN, not a CNN and achieved only a 93% accuracy. I had naively hoped that I could reach 98% or higher like some other competition winning models I have seen. The reality is, even the state of the art is well-below this target.\n",
    "\n",
    "* https://cs224d.stanford.edu/reports/TimmarajuAditya.pdf\n",
    "* https://cs224d.stanford.edu/reports/PouransariHadi.pdf\n",
    "\n",
    "\n",
    "Second, I discovered that when the parameter space is large, random search can be more effective.\n",
    "\n",
    "* http://stats.stackexchange.com/questions/160479/practical-hyperparameter-optimization-random-vs-grid-search\n",
    "* https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881#.lbesnecxm\n",
    "\n",
    "### Lessons Learned\n",
    "With every learning project, nothing is lost if I can learn something. With this project I learned to do a preliminary search of the state of the art to get the best baseline results before attempting to optimize to some specific target accuracy. I also learned that my intuition about an exhaustive grid search was well-reasoned and that there are some alternatives. While random search is not quite the advanced optimizer I had desired, it may work better than exhaustive search in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
