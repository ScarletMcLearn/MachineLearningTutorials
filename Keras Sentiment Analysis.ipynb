{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Keras Deep Learning\n",
    "\n",
    "## Credits\n",
    "Thanks to the folks at [machinelearningmastery.com](machinelearningmastery.com) for their tutorial on Sentiment Analysis for Keras Deep Learning. You can find the [original tutorial here](http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/#comment-386724).\n",
    "\n",
    "## Purpose\n",
    "My aim is to follow the tutorial and repackage it as a ready-to-go iPython notebook. That way, anyone can pull the code and interact with it without ever even leaving the tutorial. I will also provide my insights and \"what I learned\" as well. In this demo, I am also bringing the code up to date a bit. \n",
    "\n",
    "## Environment\n",
    "This notebook was written in Jupyter along with:\n",
    "* Python 3.5.2 :: Anaconda 4.1.1 (x86_64)\n",
    "* Keras (1.2.1)\n",
    "* tensorflow (0.12.1)\n",
    "* numpy (1.11.2)\n",
    "* matplotlib (1.5.3)\n",
    "\n",
    "## Overview\n",
    "\n",
    "We start with a quick visualization of the IMDB dataset, then we build two simple models one without convolutions and then one with convolutions.\n",
    "\n",
    "### Visualization & Background\n",
    "In this section of the tutorial, we write some code to better understand the dataset. \n",
    "\n",
    "#### About the Dataset\n",
    "In this case we are working with the IMDB (Internet Movie DataBase) which includes thousands of move reviews in plain text. You can [find it on Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) if you are interested in submitting your results. The words in the dataset are encoded as numbers based on their absolute popularity.\n",
    "\n",
    "The labels for the dataset are just 0 and 1 representing whether or not the review was positive.\n",
    "\n",
    "#### Imports\n",
    "As usual, we need to set matplotlib to be inline for our notebook format and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducability\n",
    "A recurring theme in data science and tutorials at [machinelearningmastery.com](machinelearningmastery.com) is reproducible research. One good rule of thumb is to initialize a random seed so that researches can repeat the same experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Data\n",
    "Now that we have everything set up, let's import the data set. Since we just want a summary of the whole data set, we can merge the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the Data\n",
    "Let's examine the dimensions, the number of possible labels (classes), the total number of unique words, and the average length of the reviews and their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n",
      "Classes: \n",
      "[0 1]\n",
      "Number of words: \n",
      "88585\n",
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))\n",
    "\n",
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))\n",
    "\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = list(map(len, X))\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Data\n",
    "We can get a better understanding of the distribution of word usage. When you look at the plot, you can see that some reviews are extremely long but they are certainly outliers. The majority appear to be somewhere just above 500. We can take advantage of that later to save time in training and to prevent overfitting to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEKCAYAAADn+anLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105b78ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.title('Review Length in Words for IMDB Dataset') # subplot 211 title\n",
    "pyplot.ylabel(\"Review Length In Words\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Word Embedding\n",
    "To preprocess, we are going to select only the most used words. We can use the nb_words parameter to tune this threshold. Tuning this treshold allows one to get the most value out of training on the least data necessary. Depending on the dataset, you might need to be careful about the value of individual words (eg cerebral, hilarious, Nicholas Cage vs. I, the, and movie).\n",
    "\n",
    "The article introduces a technique known as word embedding. In this process we are not just converting words to numbers but specifically to real-valued numbers. We can achieve this with Keras by adding an Embedding layer. We can specify the shape of the embeddings as well. Converting the review text into real-valued numbers enables us to train a neural network just like any other.\n",
    "\n",
    "It is important to note that we will truncate any large reviews and pad any small reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad the small reviews\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model and Compile to Backend\n",
    "Here is the critical code for Keras to build the net. We have a boilerplate Sequential() object to which we add() several layers including the aforementioned Embedding() as well as a Flatten() layer, a hidden Dense() layer, and a single Dense() output for a positive/negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "# Note the word embeddings\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "# Flatten (or unroll) takes us from a 2D matrix (32x500) to 1D vector (1x32*500)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually print out a description of the model object with summary(). Pretty handy! Maybe one day it will even print out a diagram of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       160000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 250)           4000250     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             251         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Fit the Model\n",
    "This step takes a few seconds to a few minutes to get an accuracy around 87%. I would love to see a comparison to human ability for this one as some reviews could be difficult to understand and would obviously take much much longer for a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.35%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=0)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Baseline Model\n",
    "Our results are promising. It is possible to imagine that with additional tuning of the word embeddings and neural network architecture, we might be able to achieve even better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Model\n",
    "Now that we have a baseline, we can experiment with other techniques. We are going to see what happens when we add a convolution. More information about convolutions can be found at [machinelearningmastery.com](machinelearningmastery.com) as well as this GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model and Compile to Backend\n",
    "The model is essentially the same but we also apply a Convolution1D() layer and a MaxPooling1D() layer. The pooling layer is akin to dimensionality reduction. Intuitively, we can think of it as a way to take only the relevant information gleaned from convolution and discarding the excess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 500, 32)       160000      embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 500, 32)       3104        embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 250, 32)       0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 8000)          0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 250)           2000250     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             251         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "This will take several minutes longer than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.38%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=0)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Applying Convolutions\n",
    "In the end, we can see at least a small improvement by using convolutions. Our author attributes the improvement from convolution to the concept of sequences within the text much like sequences/patterns in image features which conv-nets are known to be good at learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this tutorial we applied two different neural nets to movie reviews to determine if they were positive or negative with promising results. Many thanks to [machinelearningmastery.com](machinelearningmastery.com) for their well-written tutorials and (working) code examples! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
